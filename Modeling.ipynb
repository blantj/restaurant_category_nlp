{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7f7dfd8c79b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdummy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from nltk.probability import FreqDist\n",
    "from matplotlib import cm\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Yelp review data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('yelp_reviews.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrub Data: Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create numerical dummy column for review class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {'chinese': 0, 'japanese':1, 'indpak':2}\n",
    "df['Class'] = df['Category'].replace(class_map)\n",
    "df.iloc[1195:1205,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of tokenized chinese restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_reviews = list(df[df['Class'] == 0]['Review'].values)\n",
    "chinese_tokens = []\n",
    "for review in chinese_reviews:\n",
    "    chinese_tokens.extend(nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of tokenized japanese restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "japanese_reviews = list(df[df['Class'] == 1]['Review'].values)\n",
    "japanese_tokens = []\n",
    "for review in japanese_reviews:\n",
    "    japanese_tokens.extend(nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create list of tokenized indian restaurant reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_reviews = list(df[df['Class'] == 2]['Review'].values)\n",
    "indian_tokens = []\n",
    "for review in indian_reviews:\n",
    "    indian_tokens.extend(nltk.word_tokenize(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords from chinese, japanese and indian token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['.','i',\",\",'!','s','gym','barber','barbers', '...',\"'s\",\"n't\",'(',')',\"'m\",'ve',\"I've\", 'chinese',\n",
    "                  'japanese', 'indian', 'china', 'japan', 'india'])\n",
    "filtered_chinese_tokens = []\n",
    "for word in chinese_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_chinese_tokens.append(word)\n",
    "filtered_japanese_tokens = [] \n",
    "for word in japanese_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_japanese_tokens.append(word)\n",
    "filtered_indian_tokens = [] \n",
    "for word in indian_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_indian_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem gym and barber token lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "stemmed_chinese = [stemmer.stem(word) for word in filtered_chinese_tokens]\n",
    "stemmed_japanese = [stemmer.stem(word) for word in filtered_japanese_tokens]\n",
    "stemmed_indian = [stemmer.stem(word) for word in filtered_indian_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lists of 25 most frequent words and word counts for gym and barber classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_freq = FreqDist(stemmed_chinese)\n",
    "japanese_freq = FreqDist(stemmed_japanese)\n",
    "indian_freq = FreqDist(stemmed_indian)\n",
    "chinese_freq_words = [x[0] for x in chinese_freq.most_common(25)]\n",
    "chinese_freq_counts = [x[1] for x in chinese_freq.most_common(25)]\n",
    "japanese_freq_words = [x[0] for x in japanese_freq.most_common(25)]\n",
    "japanese_freq_counts = [x[1] for x in japanese_freq.most_common(25)]\n",
    "indian_freq_words = [x[0] for x in indian_freq.most_common(25)]\n",
    "indian_freq_counts = [x[1] for x in indian_freq.most_common(25)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create frequency distribution bar graphs for 25 most frequent words for gym and barber classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = cm.viridis_r(np.linspace(.4,.8, 30))\n",
    "fig, ax = plt.subplots(nrows = 3, ncols=1, figsize=(10,25))\n",
    "ax[0].bar(chinese_freq_words, chinese_freq_counts, color=color)\n",
    "ax[0].set_xticklabels(labels=chinese_freq_words, rotation=90)\n",
    "ax[0].set_title('Frequency Distribution of Top 25 Chinese Restaurant Review Tokens')\n",
    "ax[0].set_xlabel('Tokens')\n",
    "ax[0].set_ylabel('Frequency')\n",
    "ax[1].bar(japanese_freq_words, japanese_freq_counts, color=color)\n",
    "ax[1].set_xticklabels(labels=japanese_freq_words, rotation=90)\n",
    "ax[1].set_title('Frequency Distribution of Top 25 Japanese Restaurant Review Tokens')\n",
    "ax[1].set_xlabel('Tokens')\n",
    "ax[1].set_ylabel('Frequency')\n",
    "ax[2].bar(indian_freq_words, indian_freq_counts, color=color)\n",
    "ax[2].set_xticklabels(labels=indian_freq_words, rotation=90)\n",
    "ax[2].set_title('Frequency Distribution of Top 25 Indian Restaurant Review Tokens')\n",
    "ax[2].set_xlabel('Tokens')\n",
    "ax[2].set_ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create word clouds for 25 most frequent words for gym and barber classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_freq_zipped = dict(zip(chinese_freq_words, chinese_freq_counts))\n",
    "japanese_freq_zipped = dict(zip(japanese_freq_words, japanese_freq_counts))\n",
    "indian_freq_zipped = dict(zip(indian_freq_words, indian_freq_counts))\n",
    "chinese_wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(chinese_freq_zipped)\n",
    "japanese_wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(japanese_freq_zipped)\n",
    "indian_wordcloud = WordCloud(colormap='Spectral').generate_from_frequencies(indian_freq_zipped)\n",
    "fig, ax = plt.subplots(nrows=3,ncols=1, figsize=(10,30))\n",
    "ax[0].imshow(chinese_wordcloud)\n",
    "ax[0].set_title('Chinese Restaurant Reviews Top 25 Tokens Wordcloud')\n",
    "ax[0].set_xticklabels([])\n",
    "ax[0].set_yticklabels([])\n",
    "ax[1].imshow(japanese_wordcloud)\n",
    "ax[1].set_title('Japanese Restaurant Reviews Top 25 Tokens Wordcloud')\n",
    "ax[1].set_xticklabels([])\n",
    "ax[1].set_yticklabels([])\n",
    "ax[2].imshow(indian_wordcloud)\n",
    "ax[2].set_title('Indian Restaurant Reviews Top 25 Tokens Wordcloud')\n",
    "ax[2].set_xticklabels([])\n",
    "ax[2].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split Yelp review data into x and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df['Review'].values\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform train test split on Yelp review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=11, train_size = .2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize Yelp review data using tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "vectorizer.fit(x_train)\n",
    "tfidf_x_train = vectorizer.transform(x_train)\n",
    "tfidf_x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train baseline Dummy Classifier for Yelp review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(random_state=78)\n",
    "dummy.fit(tfidf_x_train, y_train)\n",
    "y_train_pred_dummy = dummy.predict(tfidf_x_train)\n",
    "y_test_pred_dummy = dummy.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred_dummy, average='weighted'))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred_dummy, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform GridSearchCV on Naive Bayes to find optimal alpha value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_dict = {'alpha': np.array(range(1,10000))/1000}\n",
    "gs_nb = MultinomialNB()\n",
    "gs_nb = GridSearchCV(gs_nb, nb_dict, scoring='f1_weighted', n_jobs=-1, cv=5, verbose=1)\n",
    "gs_nb.fit(tfidf_x_train, y_train)\n",
    "gs_nb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Naive Bayes model based on optimal alpha value from GridsearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB(alpha=.54)\n",
    "nb.fit(tfidf_x_train, y_train)\n",
    "y_train_pred = nb.predict(tfidf_x_train)\n",
    "y_test_pred = nb.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred, average='weighted'))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Confusion Matrix for Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "matrix = sns.heatmap(cm/np.sum(cm), annot=True, ax = ax, cmap = 'Blues', fmt = '.1%')\n",
    "matrix.set_title('Naive Bayes Confusion Matrix')\n",
    "matrix.set_xlabel('Predicted')\n",
    "matrix.set_xticklabels(['Barber','Gym'])\n",
    "matrix.set_ylabel('Actual')\n",
    "matrix.set_yticklabels(['Barber','Gym'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform GridSearchCV on Random Forest to find optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {'max_depth': range(25,30), 'min_samples_leaf': range(1,30), 'criterion': ['gini','entropy']}\n",
    "gs_rf = RandomForestClassifier()\n",
    "gs_rf = GridSearchCV(gs_rf, rf_dict, scoring='f1_weighted', n_jobs=-1, cv=5, verbose=1)\n",
    "gs_rf.fit(tfidf_x_train, y_train)\n",
    "gs_rf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Random Forest Classifier based on GridsearchCV results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion = 'gini', max_depth = 29, min_samples_leaf = 1)\n",
    "rf.fit(tfidf_x_train, y_train)\n",
    "y_train_pred_rf = rf.predict(tfidf_x_train)\n",
    "y_test_pred_rf = rf.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred_rf, average='weighted'))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred_rf, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Confusion Matrix for Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred_rf)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "matrix = sns.heatmap(cm/np.sum(cm), annot=True, ax = ax, cmap = 'Blues', fmt = '.1%')\n",
    "matrix.set_title('Random Forest Confusion Matrix')\n",
    "matrix.set_xlabel('Predicted')\n",
    "matrix.set_xticklabels(['Barber','Gym'])\n",
    "matrix.set_ylabel('Actual')\n",
    "matrix.set_yticklabels(['Barber','Gym'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Gradient Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(min_impurity_decrease=.0037)\n",
    "gb.fit(tfidf_x_train, y_train)\n",
    "y_train_pred_gb = gb.predict(tfidf_x_train)\n",
    "y_test_pred_gb = gb.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred_gb, average='weighted'))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred_gb, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Confusion Matrix for Gradient Boost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred_gb)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "matrix = sns.heatmap(cm/np.sum(cm), annot=True, ax = ax, cmap = 'Blues', fmt = '.1%')\n",
    "matrix.set_title('Gradient Boost Confusion Matrix')\n",
    "matrix.set_xlabel('Predicted')\n",
    "matrix.set_xticklabels(['Barber','Gym'])\n",
    "matrix.set_ylabel('Actual')\n",
    "matrix.set_yticklabels(['Barber','Gym'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gridsearch xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dict = {'min_impurity_decrease': [x/10000 for x in range(1,100)]}\n",
    "gs_xgb = XGBClassifier()\n",
    "gs_xgb = GridSearchCV(gs_xgb, xgb_dict, scoring='f1_weighted', n_jobs=-1, cv=5, verbose=1)\n",
    "gs_xgb.fit(tfidf_x_train, y_train)\n",
    "gs_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and validate top performing xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(min_impurity_decrease=0.1)\n",
    "xgb.fit(tfidf_x_train, y_train)\n",
    "y_train_pred_xgb = xgb.predict(tfidf_x_train)\n",
    "y_test_pred_xgb = xgb.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred_xgb, average='weighted'))\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred_xgb, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train voting classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VotingClassifier(estimators=[('nb', nb), ('rf', rf), ('gb', gb)], voting='soft', weights = [.615,.594,.581])\n",
    "vc.fit(tfidf_x_train, y_train)\n",
    "y_train_pred_vc = vc.predict(tfidf_x_train)\n",
    "y_test_pred_vc = vc.predict(tfidf_x_test)\n",
    "print('Train F1 Score: ', f1_score(y_train, y_train_pred_vc, average='weighted'))\n",
    "\n",
    "print('Test F1 Score: ', f1_score(y_test, y_test_pred_vc, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Confusion Matrix for Voting Classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_test_pred_vc)\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "matrix = sns.heatmap(cm/np.sum(cm), annot=True, ax = ax, cmap = 'Blues', fmt = '.1%')\n",
    "matrix.set_title('Voting Classifier Confusion Matrix')\n",
    "matrix.set_xlabel('Predicted')\n",
    "matrix.set_xticklabels(['Barber','Gym'])\n",
    "matrix.set_ylabel('Actual')\n",
    "matrix.set_yticklabels(['Barber','Gym'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
